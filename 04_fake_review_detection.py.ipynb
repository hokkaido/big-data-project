{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('big-data').master(\"local[8]\").config(\"spark.driver.memory\", \"8g\").config('spark.sql.execution.arrow.enabled', 'true').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read the generated fake reviews and try to get a sense of how long the average review is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fake_reviews = spark.read.text('fake-reviews/generated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(length(value))|\n",
      "+------------------+\n",
      "|100.48283333333333|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length, mean\n",
    "\n",
    "tokenized_fake_reviews.agg(mean(length(tokenized_fake_reviews.value))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read some real reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reviews = spark.read.json('yelp-dataset/yelp_academic_dataset_review.json').sample(False, 0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to limit reviews to around 100 characters. This is obviously not 100% exact because the generated reviews are tokenized and the yelp review data set is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "real_reviews = real_reviews.where(length(real_reviews.text) <= 100).limit(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove new-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[\\\\r\\\\n]\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[^\\x00-\\x7F]+\", \" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|pDoDY-cDLyeKQgDx5...|   0|2012-04-28 09:28:54|    0|iS0ANobtW1ZfBdu36...|  5.0|Great food. Try t...|     0|0MKwtAFrhNslL-h5E...|\n",
      "|ybbcg01-j7tKJ_oLE...|   0|2015-04-01 15:20:43|    0|Sc0nxPCLfPt7KHdN3...|  2.0|Very average Toro...|     0|GqGlMZegcGp5GKM6N...|\n",
      "|YzMUZjUMcgI-NSGu4...|   0|2014-07-13 15:06:28|    0|TJjrhzZxMfmMjlGrw...|  4.0|Food was amazon, ...|     0|xvACCLMLVs1p4Q4va...|\n",
      "|EAs61Wm1O6tLjCs8t...|   0|2015-03-18 21:06:16|    0|LbRonpdNBwxWT4Pou...|  4.0|I recommend there...|     0|pcr9Gj69fZtU5hX6O...|\n",
      "|nrahyQyopCtajDqUt...|   0|2018-04-04 02:16:24|    0|5gKt71TDpn0LyQHlf...|  5.0|I have eaten here...|     0|tLKOtNVdoJnE4xk7C...|\n",
      "|XXW_OFaYQkkGOGniu...|   0|2017-11-06 19:25:27|    0|pneLSoCx3MBveUPBy...|  5.0|Amazing decor and...|     0|EDmcD-O81-Wk8m7xT...|\n",
      "|6kcjOA5J8NFgkx7Ds...|   0|2017-09-09 00:37:49|    0|3dLDMDVQ8hIIb6Z5Y...|  5.0|Everything in the...|     0|2z3Wy-0L2pjUO8Lq4...|\n",
      "|eTXYID00jGxq1vZpn...|   0|2016-05-31 02:25:45|    0|maJnX4s13-rktjbFi...|  5.0|From San Diego an...|     0|OkI3Nao2CECx3lwZM...|\n",
      "|hXzoNgpkC86K_Jfg_...|   0|2018-03-04 00:52:09|    0|5VfyhHQf9qETc-2jJ...|  3.0|Food was just OK....|     1|R_VS8Qxm1YYCxZbXs...|\n",
      "|ck3rA4xSMx4T8QLCw...|   0|2014-08-24 21:31:04|    0|027JU78lDle-W8dJD...|  5.0|Wonderful Greek c...|     0|EOtQzVEyq-PrDrU62...|\n",
      "|lUG4i-s3zNFY1f6bE...|   0|2018-03-04 02:47:23|    0|iwikcyeyNjzTtIRd6...|  5.0|Lizzie is amazing...|     0|BiN3SMtPQhGjT0q8O...|\n",
      "|6xgcHeLad-VaoTIQe...|   0|2011-04-22 02:55:40|    0|TERPyIClY_nBcKBzy...|  5.0|5 stars, no doubt...|     0|tVxeaerNjBJN6qaOd...|\n",
      "|N0apJkxIem2E8irTB...|   1|2015-03-04 23:09:40|    1|CNwkOOKC6YBsII_ih...|  5.0|Great food and se...|     2|FgEcp4jdbNS9UY-Bm...|\n",
      "|WnLhd38sH80ViWwzy...|   0|2017-07-08 20:07:32|    0|b8ZZxw-mEYyCK0rEx...|  5.0|You're missing ou...|     0|E-ZAACaiYtfN64y2z...|\n",
      "|z9oJeVmNEc3F0ToZ0...|   1|2015-08-07 18:10:15|    1|GV4Uke_HCB--XDN-k...|  5.0|I love the macaro...|     1|BnMIEN3a5NM3BTkhA...|\n",
      "|M51gw2cz_vXarNBLb...|   0|2015-03-05 02:53:09|    0|Fv-0yuoTQJthYog8F...|  3.0|I love grabbing b...|     0|qh9-7t43_5_aFs4mw...|\n",
      "|Hh4qRAOswrmBNWt3w...|   1|2013-12-29 14:35:23|    0|btQRcC3_9cdm5VKK8...|  5.0|Very nice staff. ...|     1|Rt1FObAgPUkYw0Jkf...|\n",
      "|7tdVsSDXRKtLLC-fD...|   0|2013-07-14 00:57:45|    0|q39LbKGHP1FXmH8p6...|  4.0|Best Mexican food...|     0|QTuwtRCrsLWDlMeAL...|\n",
      "|LtXy1VinKWfuLFslV...|   0|2015-07-16 22:11:30|    0|svgJpUe5W9y1rmoSh...|  4.0|Great burgers, I ...|     0|Z0BARqjDUjteRNoY-...|\n",
      "|cyVo7B55lWegIbV9k...|   2|2012-03-01 18:57:11|    1|prrG36HPsJ30-tszr...|  4.0|Great Place for d...|     5|vgjh4QE5wfXRHuPO5...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save for tokenization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reviews.select(real_reviews.text).write.format('text').save('fake-reviews/real_reviews.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the fake_review generation notebook, the regular tokenizers are not good enough, and we have used Stanford's CoreNLP package for tokenization when training our fake review generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 111869 tokens at 691655.64 tokens per second.\r\n"
     ]
    }
   ],
   "source": [
    "!cat fake-reviews/real_reviews.txt/part-*.txt | CLASSPATH=~/dev/stanford-parser-full-2018-10-17/stanford-parser.jar java edu.stanford.nlp.process.PTBTokenizer -lowerCase -preserveLines > fake-reviews/tokenized_real_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_real_reviews = spark.read.text('fake-reviews/tokenized_real_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_real_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "tokenized_real_reviews = tokenized_real_reviews.withColumn('label', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fake_reviews = tokenized_fake_reviews.withColumn('label', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = tokenized_fake_reviews.union(tokenized_real_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = reviews.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|label|\n",
      "+--------------------+-----+\n",
      "|this place is a j...|    1|\n",
      "|this place is n't...|    0|\n",
      "|this company is t...|    1|\n",
      "|excellent sushi !...|    0|\n",
      "|i love this place...|    1|\n",
      "|this place is ok ...|    1|\n",
      "|tuesdays half of ...|    0|\n",
      "|i ordered a mediu...|    0|\n",
      "|pizza my dear has...|    0|\n",
      "|they fixed my iph...|    1|\n",
      "|do n't stay here ...|    1|\n",
      "|they are the best...|    0|\n",
      "|very efficient , ...|    0|\n",
      "|called and no ans...|    0|\n",
      "|i love this place...|    1|\n",
      "|i had a great exp...|    1|\n",
      "|the staff at chan...|    1|\n",
      "|this mexican is i...|    0|\n",
      "|the rooms are nic...|    1|\n",
      "|some of the worst...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|               value|label|\n",
      "+--------------------+-----+\n",
      "|trust me when i s...|    0|\n",
      "|i had a huge meat...|    0|\n",
      "|love this place !...|    1|\n",
      "|i love this place...|    1|\n",
      "|if you can ignore...|    0|\n",
      "|delicious food -l...|    0|\n",
      "|we had a great ti...|    1|\n",
      "|dr. concord is th...|    1|\n",
      "|great coffee !! a...|    0|\n",
      "|best beef satay n...|    0|\n",
      "|this is my favori...|    1|\n",
      "|from the other ch...|    0|\n",
      "|this place is pre...|    1|\n",
      "|good place to get...|    1|\n",
      "|it 's a good plac...|    1|\n",
      "|i had a great exp...|    1|\n",
      "|great stop today ...|    0|\n",
      "|good food , but t...|    1|\n",
      "|gave them another...|    0|\n",
      "|the food was good...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.sample(False, 0.15, seed=42).orderBy(rand()).show()\n",
    "val.sample(False, 0.15, seed=42).orderBy(rand()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "tf = HashingTF(numFeatures=2**16, inputCol=\"tokens\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=4)\n",
    "pipeline = Pipeline(stages=[tokenizer, tf, idf])\n",
    "\n",
    "idf_model = pipeline.fit(train)\n",
    "train_df = idf_model.transform(train)\n",
    "val_df = idf_model.transform(val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100, labelCol='label')\n",
    "lr_model = lr.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = lr_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under Curve / ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781689747598837"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a good Area under Curve / ROC. This is very dangerous however since in real life fake and real reviews are imbalanced. There are fewer fake reviews than real reviews and we should thus use the f1-score or Area Under Curve / PR instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748998646699216"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predict_test, {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create an imbalanced test set, the Area under the curve for precision-recall and the f1-score drops significantly few a 'few fake reviews' / 'a lot of real reviews' scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(df):\n",
    "    tp = df.where((df.label == 1) & (df.prediction == 1)).count()\n",
    "    tn = df.where((df.label == 0) & (df.prediction == 0)).count()\n",
    "    fp = df.where((df.label == 0) & (df.prediction == 1.0)).count()\n",
    "    fn = df.where((df.label == 1) & (df.prediction == 0)).count()\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779574970484071\n",
      "0.9521065292036168\n",
      "0.8944723618090452\n"
     ]
    }
   ],
   "source": [
    "imbalanced_val = val.where(val.label==1).sample(0.5).union(val.where(val.label==0))\n",
    "predict_imbalanced = lr_model.transform(idf_model.transform(imbalanced_val))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predict_imbalanced))\n",
    "print(evaluator.evaluate(predict_imbalanced, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print(f1_score(predict_imbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775343340082756\n",
      "0.985803527150516\n",
      "0.9567430025445293\n"
     ]
    }
   ],
   "source": [
    "imbalanced_val = val.where(val.label==0).sample(0.5).union(val.where(val.label==1))\n",
    "predict_imbalanced = lr_model.transform(idf_model.transform(imbalanced_val))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predict_imbalanced))\n",
    "print(evaluator.evaluate(predict_imbalanced, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print(f1_score(predict_imbalanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this test set isn't that large. We can try loading another 10k fake reviews which were created from context inputs the trained model has never seen. We can then try to achieve a 20:80 ratio of fake to real reviews (bear in mind we have no way of knowing whether the Yelp data set does not contain any fake reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_reviews = spark.read.json('yelp-dataset/yelp_academic_dataset_review.json').sample(False, 0.4, seed=42)\n",
    "real_reviews = real_reviews.where(length(real_reviews.text) <= 100).limit(40000)\n",
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[\\\\r\\\\n]\", \" \"))\n",
    "real_reviews = real_reviews.withColumn('text', regexp_replace(real_reviews.text, \"[^\\x00-\\x7F]+\", \" \"))\n",
    "real_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 748407 tokens at 1647591.67 tokens per second.\r\n"
     ]
    }
   ],
   "source": [
    "real_reviews.select(real_reviews.text).write.format('text').save('fake-reviews/real_reviews_40k.txt')\n",
    "!cat fake-reviews/real_reviews_40k.txt/part-*.txt | CLASSPATH=~/dev/stanford-parser-full-2018-10-17/stanford-parser.jar java edu.stanford.nlp.process.PTBTokenizer -lowerCase -preserveLines > fake-reviews/tokenized_real_reviews_40k.txt\n",
    "tokenized_real_reviews = spark.read.text('fake-reviews/tokenized_real_reviews_40k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fake_reviews = spark.read.text('fake-reviews/generated_10k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_real_reviews = tokenized_real_reviews.withColumn('label', lit(0))\n",
    "tokenized_fake_reviews = tokenized_fake_reviews.withColumn('label', lit(1))\n",
    "test_reviews = tokenized_fake_reviews.union(tokenized_real_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9749909325000622\n",
      "0.8945515982757801\n",
      "0.8344529963963163\n"
     ]
    }
   ],
   "source": [
    "predict_test = lr_model.transform(idf_model.transform(test_reviews))\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predict_test))\n",
    "print(evaluator.evaluate(predict_test, {evaluator.metricName: \"areaUnderPR\"}))\n",
    "print(f1_score(predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at first glance, the F1-Score isn't all that bad, we can also look at the averaged F1-Score and print the underlying statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_(df, p=1, n=0):\n",
    "    tp = df.where((df.label == p) & (df.prediction == p)).count()\n",
    "    tn = df.where((df.label == n) & (df.prediction == n)).count()\n",
    "    fp = df.where((df.label == n) & (df.prediction == p)).count()\n",
    "    fn = df.where((df.label == p) & (df.prediction == n)).count()\n",
    "    print('tp {}, fp {}, tn {}, fn {}'.format(tp, fp, tn, fn))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print('precision', precision)\n",
    "    print('recall', recall)\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp 9378, fp 3099, tn 36901, fn 622\n",
      "precision 0.7516229862947824\n",
      "recall 0.9378\n",
      "tp 36901, fp 622, tn 9378, fn 3099\n",
      "precision 0.983423500253178\n",
      "recall 0.922525\n",
      "avg f1 0.8932271689668332\n"
     ]
    }
   ],
   "source": [
    "print('avg f1', (f1_score_(predict_test, 1, 0) + f1_score_(predict_test, 0, 1))/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the precision and recall, the question is whether we could live with these high misclassification rates in a production environment. What is more important: preventing as many fake reviews as possible, or letting through some fake reviews in order to not annoy users?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
